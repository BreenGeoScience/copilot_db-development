# Instructions: How to Guide Chat Assistance to Fuzzy Allocation Mapping Cleanup

## Purpose
These instructions walk you through the process of preparing large vendor/keyword mapping CSVs in Chat (GitHub Copilot Chat), deduplicating and broadening fuzzy mapping—arriving at the step where you export a 2-column, cleaned, space-trimmed CSV of broad keywords with account codes.

---

## Step-by-step Workflow to Reach This Point

### 1. **Initial Setup**
- Have your transactions/vendor mapping file ready (e.g., `cleaned_fuzzymap_import.csv`).
- File has columns: `keyword,gl_account_code`.

### 2. **Import to Database (Optional)**
- Import your raw mapping file if you want to inspect contents in SQL, or work only with the CSV in scripts/spreadsheets.

### 3. **Spot Unmapped/Problem Rows**
- Review entries with "TODO" or blank `gl_account_code`.
- Use spreadsheet filters, shell commands, or SQL queries.

### 4. **Discussion of Vendor Name Fuzziness**
- Discuss (with assistant) the challenge of matching vendors that may appear with variable store numbers, IDs, cities, or punctuation (e.g., "BIG LOTS STORE #117", "BIGBBY COFFEE #351 SAULT SAINTE").

### 5. **Decide on Fuzzy vs Regex Strategy**
- Determine whether plain substring or (preferably) regex should be used for matching.
- Choose to “normalize” vendor names for broad mapping.
  - Strip digits, store numbers, city/state, IDs from `keyword`.

### 6. **Script/Terminal Command Development**
- Request and review Python or shell script to:
  - Extract & normalize vendor keywords to the broadest useful form.
  - Optionally, keep both original columns.
  - Trim any spaces before/after both columns.
- Example shell/awk code to output:
  - `broad_keyword,gl_account_code`
  - Both columns **trimmed**, deduplicated.

### 7. **Final Cleaned CSV Output**
- Save the output as e.g., `broad_map_deduped.csv`.

### 8. **Instruction Export Point**
- To record or share, export these or similar markdown instructions as `20251110_161000_INSTRUCTIONS.md`.

---

## Example Terminal Command (from Chat)
```sh
awk -F',' 'NR==1 {print "broad_keyword,gl_account_code"; next}
{
  # Trim and normalize keyword
  kw=$1; gsub(/^ +| +$/, "", kw)
  acc=$2; gsub(/^ +| +$/, "", acc)
  broad=kw
  gsub(/ - *#?[0-9]+/,"", broad)
  gsub(/ #[0-9]+/,"", broad)
  gsub(/[0-9]+/,"", broad)
  broad=toupper(broad)
  gsub(/  +/," ", broad)
  broad=gensub(/^ +| +$/, "", "g", broad)
  print broad "," acc
}' cleaned_fuzzymap_import.csv | sort | uniq > broad_map_deduped.csv
```

---

## What To Tell Chat (Copilot) to Get Here

**Instruct Chat as follows:**
1. I have a vendor keyword mapping CSV (keyword, gl_account_code, 1500+ entries).
2. Many have store numbers, IDs, or city suffixes in the keyword.
3. I want to deduplicate and “broaden” the keyword mapping: remove/ignore numbers, store/city designators to produce a broad mapping.
4. Give me UNIX terminal commands (not Python) to:
    - Normalize keywords as above.
    - Output both columns, trimmed, to a new CSV.
    - Deduplicate.
5. Repeat back the precise awk/sort/uniq pipeline for my shell.

---

**That will bring chat to the point of presenting the above one-liner and cleaning workflow, as shown.**
